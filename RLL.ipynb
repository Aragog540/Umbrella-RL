{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52541a2a-b1a9-4d77-b48e-4e7d334ca3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Policy Network (Actor)\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return torch.tanh(self.net(state))  # Continuous actions\n",
    "\n",
    "# Q-Network (Critic)\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        return self.net(torch.cat([state, action], dim=-1))\n",
    "\n",
    "# Adaptive Entropy Coefficient (Î±)\n",
    "class EntropyCoefficient(nn.Module):\n",
    "    def __init__(self, init_alpha=0.2):\n",
    "        super().__init__()\n",
    "        self.log_alpha = nn.Parameter(torch.tensor(np.log(init_alpha)))\n",
    "    \n",
    "    def forward(self):\n",
    "        return torch.exp(self.log_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d203408d-4d04-46b7-85ef-77ce7a7e7375",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaPolicy(nn.Module):\n",
    "    def __init__(self, state_dim, goal_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, goal_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.net(state)  # Output sub-goal\n",
    "\n",
    "# Hindsight Experience Replay (HER)\n",
    "def relabel_trajectory(trajectory, achieved_goals):\n",
    "    new_goals = achieved_goals[-1].unsqueeze(0)  # Use final achieved goal as new target\n",
    "    relabeled_trajectory = []\n",
    "    for transition in trajectory:\n",
    "        state, action, reward, next_state, done = transition\n",
    "        new_reward = compute_reward(next_state, new_goals)  # Custom reward function\n",
    "        relabeled_trajectory.append((state, action, new_reward, next_state, done))\n",
    "    return relabeled_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0777a784-af7c-456a-9ee6-aa2ef870be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(q_net, policy_net, alpha, states, actions, rewards, next_states, dones, gamma=0.99):\n",
    "    with torch.no_grad():\n",
    "        next_actions = policy_net(next_states)\n",
    "        next_q = q_net(next_states, next_actions)\n",
    "        target_q = rewards + gamma * (1 - dones) * next_q\n",
    "    \n",
    "    current_q = q_net(states, actions)\n",
    "    q_loss = nn.MSELoss()(current_q, target_q)\n",
    "    \n",
    "    # Policy Loss with Entropy Regularization\n",
    "    new_actions = policy_net(states)\n",
    "    new_q = q_net(states, new_actions)\n",
    "    entropy = -alpha * policy_net.log_prob(new_actions)  # Assuming policy outputs log_std\n",
    "    policy_loss = -(new_q + entropy).mean()\n",
    "    \n",
    "    # Entropy Coefficient Loss\n",
    "    alpha_loss = -alpha * (policy_net.log_prob(new_actions) + target_entropy).detach().mean()\n",
    "    \n",
    "    return q_loss, policy_loss, alpha_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57237305-d0f8-4af7-ac68-c4e21ece662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize networks\n",
    "policy_net = PolicyNetwork(state_dim, action_dim)\n",
    "q_net1 = QNetwork(state_dim, action_dim)\n",
    "q_net2 = QNetwork(state_dim, action_dim)  # Double Q-learning\n",
    "alpha_net = EntropyCoefficient()\n",
    "meta_policy = MetaPolicy(state_dim, goal_dim)\n",
    "\n",
    "optimizer_policy = optim.Adam(policy_net.parameters(), lr=3e-4)\n",
    "optimizer_q = optim.Adam(list(q_net1.parameters()) + list(q_net2.parameters()), lr=3e-4)\n",
    "optimizer_alpha = optim.Adam([alpha_net.log_alpha], lr=3e-4)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Collect trajectory using hierarchical policy\n",
    "    trajectory, achieved_goals = generate_trajectory(policy_net, meta_policy, env)\n",
    "    relabeled_trajectory = relabel_trajectory(trajectory, achieved_goals)\n",
    "    \n",
    "    # Update Q-networks\n",
    "    for state, action, reward, next_state, done in relabeled_trajectory:\n",
    "        q_loss1 = compute_loss(q_net1, ...)\n",
    "        q_loss2 = compute_loss(q_net2, ...)\n",
    "        optimizer_q.zero_grad()\n",
    "        (q_loss1 + q_loss2).backward()\n",
    "        optimizer_q.step()\n",
    "    \n",
    "    # Update policy and entropy coefficient\n",
    "    policy_loss, alpha_loss = compute_loss(...)\n",
    "    optimizer_policy.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    optimizer_policy.step()\n",
    "    optimizer_alpha.zero_grad()\n",
    "    alpha_loss.backward()\n",
    "    optimizer_alpha.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3815a8-346b-4e15-8499-5075e5a1f8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "env = MultiValleyMountainCar()  # Custom environment\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "# Training\n",
    "train(env, policy_net, q_net1, q_net2, alpha_net, meta_policy)\n",
    "\n",
    "# Evaluation\n",
    "mean_reward = evaluate(policy_net, env, num_episodes=100)\n",
    "print(f\"Mean Reward: {mean_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
